{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2\n",
    "NOTA:\n",
    "- In questa fase, le immagini da inserire nel database saranno etichettate come image-X-Y- Z.png, dove\n",
    "    - X ∈ {cc, con, detail, emboss, jitter, neg, noise1, noise2, original, poster, rot, smooth, stipple} denota il tipo dell’immagine;\n",
    "    - 1≤Y≤40denota ilsubjectID,and\n",
    "    - 1 ≤ Z ≤ 10 denota l’ ID del campione dell’immagine.\n",
    "    \n",
    "\n",
    "- I task in questa fase riguardano i tre modelli di feature e le misure di distanza/similarita’ sviluppate nella precedente.\n",
    " \n",
    "- Potete usare le librerie esistenti per la decomposizione LDA.\n",
    "- Potete usare le librerie esistenti per l’estrazione di autovalori e autovettori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.task1_new import *\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.stats import skew\n",
    "\n",
    "# load and display an image with Matplotlib\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import re\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787\n"
     ]
    }
   ],
   "source": [
    "# import OS module\n",
    "import os\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"./secondaparte/images/\"\n",
    "dir_list = os.listdir(path) # array di stringhe contenente i nomi dei file\n",
    " \n",
    "dir_list.sort()\n",
    "dir_list.remove('.DS_Store')\n",
    "\n",
    "print(len(dir_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryAndWriteOnFile(matrix, filtered_dir_list, passedParameter, strategyName, modelName):\n",
    "    matrixtrasposta = matrix.T\n",
    "\n",
    "    dictionaryFeaturesLatenti = {}\n",
    "    for i in range(len(matrixtrasposta)):\n",
    "        sequenzaFeatureLatente = []\n",
    "        for j in range(len(matrixtrasposta[i])):\n",
    "            splittedString = re.split('-', filtered_dir_list[j])\n",
    "            pair = ()\n",
    "\n",
    "            if passedParameter == 'X':\n",
    "                pair = (splittedString[2], matrixtrasposta[i][j])\n",
    "            elif passedParameter == 'Y':\n",
    "                pair = (splittedString[1], matrixtrasposta[i][j])\n",
    "            \n",
    "            sequenzaFeatureLatente.append(pair)\n",
    "        \n",
    "        sequenzaOrdinata = sorted(sequenzaFeatureLatente, key=lambda x: x[1], reverse=True)\n",
    "        dictionaryFeaturesLatenti['Feature' + str(i)] = sequenzaOrdinata\n",
    "\n",
    "    #print(dictionaryFeaturesLatenti)\n",
    "\n",
    "    fileName = ''\n",
    "\n",
    "    if passedParameter == 'X':\n",
    "        fileName = 'VISUAL_task1_' + modelName + '_' + strategyName + '_coppie_soggetto-peso.txt'\n",
    "    elif passedParameter == 'Y':\n",
    "        fileName = 'VISUAL_task2_' + modelName + '_' + strategyName + '_coppie_tipo-peso.txt'\n",
    "\n",
    "    # controlliamo se il file esiste o no\n",
    "    file = open('./outputfilesecondaparte/' + fileName,'w+')\n",
    "\n",
    "    with open('./outputfilesecondaparte/' + fileName, 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dictionaryFeaturesLatenti, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryAndWriteOnFileTask34(matrix, filtered_dir_list, passedParameter, strategyName, modelName):\n",
    "    matrixtrasposta = matrix.T\n",
    "\n",
    "    print(filtered_dir_list)\n",
    "    print(\"LEN LIST\", len(filtered_dir_list))\n",
    "    print(\"len(matrixtrasposta)\", len(matrixtrasposta))\n",
    "    print(\"len(matrixtrasposta[0])\", len(matrixtrasposta[0]))\n",
    "\n",
    "    dictionaryFeaturesLatenti = {}\n",
    "    for i in range(len(matrixtrasposta)):\n",
    "        sequenzaFeatureLatente = []\n",
    "        for j in range(len(matrixtrasposta[i])):\n",
    "            pair = ()\n",
    "\n",
    "            if passedParameter == 'soggetto':\n",
    "                pair = (filtered_dir_list[j], matrixtrasposta[i][j])\n",
    "            elif passedParameter == 'tipo':\n",
    "                pair = (filtered_dir_list[j], matrixtrasposta[i][j])\n",
    "            \n",
    "            sequenzaFeatureLatente.append(pair)\n",
    "        \n",
    "        sequenzaOrdinata = sorted(sequenzaFeatureLatente, key=lambda x: x[1], reverse=True)\n",
    "        dictionaryFeaturesLatenti['Feature' + str(i)] = sequenzaOrdinata\n",
    "\n",
    "    print(\"DOPO IL FOR\")\n",
    "\n",
    "    #print(dictionaryFeaturesLatenti)\n",
    "\n",
    "    fileName = ''\n",
    "\n",
    "    if passedParameter == 'soggetto':\n",
    "        fileName = 'VISUAL_task4_' + modelName + '_' + strategyName + '_coppie_soggetto-peso.txt'\n",
    "    elif passedParameter == 'tipo':\n",
    "        fileName = 'VISUAL_task3_' + modelName + '_' + strategyName + '_coppie_tipo-peso.txt'\n",
    "\n",
    "    # controlliamo se il file esiste o no\n",
    "    file = open('./outputfilesecondaparte/' + fileName,'w+')\n",
    "\n",
    "    with open('./outputfilesecondaparte/' + fileName, 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dictionaryFeaturesLatenti, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAllMatrixOnFile(u, s, vt, lista, strategyName, modelName, param, passedParameter):\n",
    "    dictionary = {}\n",
    "\n",
    "    if isinstance(u, list):\n",
    "        dictionary[\"u\"] = u\n",
    "    else:\n",
    "        dictionary[\"u\"] = u.tolist()\n",
    "\n",
    "    if isinstance(s, list):\n",
    "        dictionary[\"s\"] = s\n",
    "    else:\n",
    "        dictionary[\"s\"] = s.tolist()\n",
    "\n",
    "    if isinstance(vt, list):\n",
    "        dictionary[\"vt\"] = vt\n",
    "    else:\n",
    "        dictionary[\"vt\"] = vt.tolist()\n",
    "\n",
    "    dictionary[\"listaFiles\"] = lista\n",
    "\n",
    "    if passedParameter == 'X':\n",
    "        fileName = 'LATENT_task1_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.txt'\n",
    "    elif passedParameter == 'Y':\n",
    "        fileName = 'LATENT_task2_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.txt'\n",
    "\n",
    "    # controlliamo se il file esiste o no\n",
    "    file = open('./outputfilesecondaparte/latent/' + fileName, 'w+')\n",
    "\n",
    "    with open('./outputfilesecondaparte/latent/' + fileName, 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dictionary, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDecompositionOnFile(decomposition, strategyName, modelName, param, passedParameter):\n",
    "    if passedParameter == 'X':\n",
    "        fileName = 'LATENT_task1_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.pickle'\n",
    "    elif passedParameter == 'Y':\n",
    "        fileName = 'LATENT_task2_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.pickle'\n",
    "\n",
    "    with open('./outputfilesecondaparte/latent/' + fileName, 'wb') as f:\n",
    "        pickle.dump(decomposition, f)\n",
    "\n",
    "\n",
    "def saveDecompositionOnFileTask34(decomposition, strategyName, modelName, param):\n",
    "    if param == 'tipo':\n",
    "        fileName = 'LATENT_task3_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.pickle'\n",
    "    elif param == 'soggetto':\n",
    "        fileName = 'LATENT_task4_' + modelName + \\\n",
    "            '_' + strategyName + '_' + param + '.pickle'\n",
    "\n",
    "    with open('./outputfilesecondaparte/latent/' + fileName, 'wb') as f:\n",
    "        pickle.dump(decomposition, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def ottieniKSemanticheLatenti(modelName, param, k, strategyName, passedParameter):\n",
    "\n",
    "    filtered_dir_list = []\n",
    "\n",
    "    if passedParameter == 'X':\n",
    "        filtered_dir_list = list(filter(lambda k: param in k, dir_list))\n",
    "    elif passedParameter == 'Y':\n",
    "        for i in range(len(dir_list)):\n",
    "            splittedString = re.split('-', dir_list[i])\n",
    "\n",
    "            if splittedString[2] == param:\n",
    "                filtered_dir_list.append(dir_list[i])\n",
    "\n",
    "    print(\"Numero di file su cui applichiamo i metodi: \", len(filtered_dir_list))\n",
    "    #print(filtered_dir_list)\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    matrixObjFeatures = []\n",
    "\n",
    "    lista = []\n",
    "\n",
    "    for key in dictionaryOfDescriptors:\n",
    "        matrixObjFeatures.append(dictionaryOfDescriptors[key])\n",
    "        lista.append(key)\n",
    "\n",
    "    print(\"Lunghezza di un descrittore: \", len(matrixObjFeatures[0]))\n",
    "    print(\"Numero di oggetti: \", len(matrixObjFeatures))\n",
    "    \n",
    "    # calcoliamo il numero massimo di features latenti possibili, ovvero il minimo tra il numero di oggetti e di features di partenza\n",
    "    numberOfFeatures = min(len(matrixObjFeatures[0]), len(matrixObjFeatures))\n",
    "\n",
    "    match strategyName:\n",
    "        case 'PCA':\n",
    "            print(\"SONO IN PCA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(matrixObjFeatures)\n",
    "\n",
    "            covMatr = np.cov(numpyMatrixObjFeatures.T)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di covarianza: \", len(covMatr))\n",
    "            print(\"Numero colonne matrice di covarianza: \", len(covMatr[0]))\n",
    "            print(\"La matrice di covarianza è quadrata? \", len(covMatr) == len(covMatr[0]))\n",
    "\n",
    "            test_pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data_test = test_pca.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFile(converted_data_test, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data = pca.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFile(converted_data, strategyName, modelName, param, passedParameter)\n",
    "\n",
    "            return 1\n",
    "\n",
    "        case 'SVD':\n",
    "\n",
    "            print(\"SONO IN SVD\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(matrixObjFeatures)\n",
    "\n",
    "            test_svd = TruncatedSVD(algorithm = 'randomized', n_components = numberOfFeatures - 1)  # Abbiamo messo il -1 pechè voleva necessariamente un numero minore rispetto al numero di features iniziali\n",
    "            converted_data_test = test_svd.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFile(converted_data_test, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            svd = TruncatedSVD(algorithm = 'randomized', n_components = numberOfFeatures - 1)   # Abbiamo messo il -1 pechè voleva necessariamente un numero minore rispetto al numero di features iniziali\n",
    "            converted_data = svd.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFile(converted_data, strategyName, modelName, param, passedParameter)\n",
    "\n",
    "            return 2\n",
    "\n",
    "        case 'LDA':\n",
    "            print(\"SONO IN LDA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(matrixObjFeatures)\n",
    "            \n",
    "            test_lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "\n",
    "            converted_data_test = test_lda.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFile(converted_data_test, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "            converted_data = lda.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFile(converted_data, strategyName, modelName, param, passedParameter)\n",
    "\n",
    "            return 3\n",
    "\n",
    "        case _:\n",
    "            return 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 3 e 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def ottieniKSemanticheLatentiTask34(modelName, param, k, strategyName):\n",
    "\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    lista = []\n",
    "    #dictionaryOfDescriptors contiene TUTTI i descrittori di TUTTI gli oggetti\n",
    "    if param == 'tipo':\n",
    "        lista = ['cc', 'con', 'emboss', 'jitter', 'neg', 'noise01', 'noise02', 'original', 'poster', 'rot', 'smooth', 'stipple']\n",
    "        print(lista)\n",
    "\n",
    "    elif param == 'soggetto':\n",
    "        lista = []\n",
    "        for i in range(40):\n",
    "            lista.append(str(i)) #aggiungo il soggetto\n",
    "\n",
    "        print(lista)\n",
    "    else: \n",
    "        raise Exception(\"Param ha un valore sbagliato\")\n",
    "        \n",
    "\n",
    "    # Per poter calcolare le matrici di similarità tipo-tipo o soggetto-soggetto abbiamo deciso di seguire il seguente approccio:\n",
    "        # - presi tutti i descrittori delle immagini dello stesso tipo (o soggetto a seconda del caso) abbiamo calcolato il centroide,\n",
    "        #   il quale viene usato come rappresentante del tipo (o soggetto)\n",
    "        # - una volta calcolati tutti i centroidi abbiamo calcolato la similarità tra le varie coppie costruendo la matrice di similarità\n",
    "\n",
    "\n",
    "    # Al posto di usare i centroidi avremmo potuto usare i medoidi: ovvero gli oggetti reali del Db che distano meno dagli altri.\n",
    "\n",
    "\n",
    "    arrayDiCentroidi = []\n",
    "\n",
    "    filteredFile = []\n",
    "    for i in range(len(lista)):\n",
    "        filteredDictionary = {}\n",
    "        for key in dictionaryOfDescriptors:\n",
    "            if lista[i] in key:\n",
    "                filteredFile.append(key)\n",
    "                filteredDictionary[key] = dictionaryOfDescriptors[key]\n",
    "        \n",
    "        matrix = []\n",
    "        #TODO: calcolare il centroide\n",
    "        for key in filteredDictionary:\n",
    "            matrix.append(filteredDictionary[key])\n",
    "        \n",
    "        npMatrix = np.array(matrix)\n",
    "\n",
    "        centroide = np.mean(npMatrix, axis=0)\n",
    "        arrayDiCentroidi.append(centroide)\n",
    "\n",
    "    print(arrayDiCentroidi)\n",
    "    print(len(arrayDiCentroidi[0]))\n",
    "\n",
    "    #ciclare tutti i centroidi e calcolare le distanze tra di essi\n",
    "\n",
    "    matrixForDecomposition = np.eye(len(arrayDiCentroidi)) #matrice tipo-tipo o soggetto-soggetto\n",
    "\n",
    "    for i in range(len(arrayDiCentroidi)):\n",
    "        for j in range(i, len(arrayDiCentroidi)):\n",
    "            similarity = cosine_similarity(arrayDiCentroidi[i].reshape(1,-1), arrayDiCentroidi[j].reshape(1,-1)) # calcolo la similarità tra i due descrittori\n",
    "            matrixForDecomposition[i][j] = similarity\n",
    "            matrixForDecomposition[j][i] = similarity\n",
    "\n",
    "    print(matrixForDecomposition)\n",
    "    print(\"QUA 0\")\n",
    "\n",
    "    numpyMatrixForDecomposition = np.array(matrixForDecomposition)\n",
    "    print(\"QUA 1\")\n",
    "\n",
    "    # calcoliamo il numero massimo di features latenti possibili, ovvero il minimo tra il numero di oggetti e di features di partenza\n",
    "    numberOfFeatures = min(len(numpyMatrixForDecomposition[0]), len(numpyMatrixForDecomposition))\n",
    "    \n",
    "    # print(\"FILTERED FILE LEN: \", len(filteredFile))\n",
    "    # print(filteredFile)\n",
    "\n",
    "    print(\"PRIMA DELLO SWITCH\")\n",
    "\n",
    "    match strategyName:\n",
    "        case 'PCA':\n",
    "            print(\"SONO IN PCA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(numpyMatrixForDecomposition)\n",
    "\n",
    "            covMatr = np.cov(numpyMatrixObjFeatures.T)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di covarianza: \", len(covMatr))\n",
    "            print(\"Numero colonne matrice di covarianza: \", len(covMatr[0]))\n",
    "            print(\"La matrice di covarianza è quadrata? \", len(covMatr) == len(covMatr[0]))\n",
    "\n",
    "            test_pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data_test = test_pca.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(converted_data_test, lista, param, strategyName, modelName)\n",
    "\n",
    "            pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data = pca.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFileTask34(converted_data, strategyName, modelName, param)\n",
    "\n",
    "            return 1\n",
    "\n",
    "        case 'SVD':\n",
    "\n",
    "            print(\"SONO IN SVD\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(numpyMatrixForDecomposition)\n",
    "\n",
    "            test_svd = TruncatedSVD(algorithm = 'randomized', n_components = numberOfFeatures - 1)  # Abbiamo messo il -1 pechè voleva necessariamente un numero minore rispetto al numero di features iniziali\n",
    "            converted_data_test = test_svd.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(converted_data_test, lista, param, strategyName, modelName)\n",
    "\n",
    "            svd = TruncatedSVD(algorithm = 'randomized', n_components = numberOfFeatures - 1)   # Abbiamo messo il -1 pechè voleva necessariamente un numero minore rispetto al numero di features iniziali\n",
    "            converted_data = svd.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFileTask34(converted_data, strategyName, modelName, param)\n",
    "\n",
    "            return 2\n",
    "\n",
    "        case 'LDA':\n",
    "            print(\"SONO IN LDA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(numpyMatrixForDecomposition)\n",
    "            \n",
    "            test_lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "\n",
    "            converted_data_test = test_lda.fit_transform(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(converted_data_test, lista, param, strategyName, modelName)\n",
    "\n",
    "            lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "            converted_data = lda.fit(numpyMatrixObjFeatures)  # questa implementazione non richiede in input la matrice di covarianza\n",
    "\n",
    "            saveDecompositionOnFileTask34(converted_data, strategyName, modelName, param)\n",
    "\n",
    "            return 3\n",
    "\n",
    "        case _:\n",
    "            raise Exception(\"Metodo di decomposizione non valido\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Soluzione scelta:** Per ottenere le matrici di similarità tipo-tipo o soggetto-soggetto abbiamo recuperato tutte le immagini di quel tipo o di quel soggetto a seconda del caso e abbiamo calcolato per ogni tipo o soggetto un centroide.\n",
    "Grazie a questi centroidi abbiamo poi costruito la matrice di similarità sfruttando la cosine similarity.\n",
    "\n",
    "> **Nota bene:** Questa soluzione implementata per costruire la matrice di similarità è solo una delle tante disponibili. Avremmo potuto usare i medoidi oppure costruire una matrice per ogni tipo o soggetto e poi confrontare nella maniera corretta (perchè il confronto di matrici è soggetto all'ordine degli oggetti in esse) le matrici e calcolarne la distanza/similarit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNsimilarImages(imageQueryFile, latentSemanticFileName, useCosineSimilarity):\n",
    "\n",
    "    # apriamo l'immagine\n",
    "    image = Image.open(imageQueryFile)\n",
    "\n",
    "    try:\n",
    "        splittedName = latentSemanticFileName.split(\"_\")\n",
    "\n",
    "        typeOfDescriptor = splittedName[2] # contiene il tipo di descrittore\n",
    "\n",
    "        newImageDescriptor = ottieniDescrittoreOfAnyImages(imageQueryFile, typeOfDescriptor)\n",
    "\n",
    "        with open(latentSemanticFileName, 'rb') as f:\n",
    "            decomposition = pickle.load(f)\n",
    "\n",
    "        tmp = decomposition.transform([newImageDescriptor])\n",
    "        descriptorInNewSpace = tmp[0]\n",
    "                    \n",
    "        # Opening JSON file\n",
    "        f = open('./databasefilesecondaparte/DB_' + typeOfDescriptor + '.json')\n",
    "        \n",
    "        # returns JSON object as\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "\n",
    "        mostSimilar = []\n",
    "\n",
    "        # print(\"QUERY: ORA STAMPO IL DESCRITTORE RISPETTO ALLE K FEATURES LATENTI\")\n",
    "        # print(descriptorInNewSpace)\n",
    "\n",
    "        for key in data:\n",
    "            current_image = decomposition.transform([data[key]])\n",
    "            currentDescriptorInNewSpace = current_image[0] \n",
    "\n",
    "            # print(\"OBJECT: ORA STAMPO IL DESCRITTORE RISPETTO ALLE K FEATURES LATENTI\")\n",
    "            # print(currentDescriptorInNewSpace)\n",
    "                \n",
    "\n",
    "            if useCosineSimilarity:\n",
    "                distance = cosine_similarity(np.array(descriptorInNewSpace).reshape(1,-1), np.array(currentDescriptorInNewSpace).reshape(1,-1)) # calcolo la similarità tra i due descrittori\n",
    "            else:        \n",
    "                distance = np.linalg.norm(np.array(descriptorInNewSpace) - np.array(currentDescriptorInNewSpace))\n",
    "            \n",
    "            pair = (key, distance)\n",
    "\n",
    "            mostSimilar.append(pair)\n",
    "\n",
    "        #print(\"\\nSTAMPO  IN ORDINE TUTTE LE IMMAGINI CON LA SIMILARITA'\\n\")\n",
    "        \n",
    "        if useCosineSimilarity:\n",
    "            sequenzaOrdinata = sorted(mostSimilar, key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            sequenzaOrdinata = sorted(mostSimilar, key=lambda x: x[1])\n",
    "        #print(sequenzaOrdinata)\n",
    "\n",
    "    except:\n",
    "        print(\"Il file di query e/o quello delle semantiche latenti non esiste! Inserire un nome corretto!\")\n",
    "\n",
    "    return sequenzaOrdinata\n",
    "\n",
    "    # if done:\n",
    "    #     # convertiamo l'immagine in un numpy array\n",
    "    #     data = asarray(image)\n",
    "    #     print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 e 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import a groupby() method\n",
    "# from itertools module\n",
    "from itertools import groupby\n",
    "  \n",
    "# # dictionary\n",
    "# INFO = [\n",
    "#     {'employee': 'XYZ_1', 'company': 'ABC_1'},\n",
    "#     {'employee': 'XYZ_2', 'company': 'ABC_2'},\n",
    "#     {'employee': 'XYZ_3', 'company': 'ABC_3'},\n",
    "#     {'employee': 'XYZ_4', 'company': 'ABC_3'},\n",
    "#     {'employee': 'XYZ_5', 'company': 'ABC_2'},\n",
    "#     {'employee': 'XYZ_6', 'company': 'ABC_3'},\n",
    "#     {'employee': 'XYZ_7', 'company': 'ABC_1'},\n",
    "#     {'employee': 'XYZ_8', 'company': 'ABC_2'},\n",
    "#     {'employee': 'XYZ_9', 'company': 'ABC_1'}\n",
    "# ]\n",
    "  \n",
    "# define a fuction for key\n",
    "def key_func(k):\n",
    "    return k[0]\n",
    "  \n",
    "# # sort INFO data by 'company' key.\n",
    "# INFO = sorted(INFO, key=key_func)\n",
    "\n",
    "def setClassForImage(imageQueryFile, latentSemanticFileName, nTask):\n",
    "\n",
    "    print(\"1\")\n",
    "    done = False\n",
    "\n",
    "    # apriamo l'immagine\n",
    "    image = Image.open(imageQueryFile)\n",
    "    print(\"2\")\n",
    "    # try:\n",
    "    \n",
    "    splittedName = latentSemanticFileName.split(\"_\")\n",
    "    print(\"3\")\n",
    "    typeOfDescriptor = splittedName[2] # contiene il tipo di descrittore\n",
    "    print(\"4\")\n",
    "    newImageDescriptor = ottieniDescrittoreOfAnyImages(imageQueryFile, typeOfDescriptor)\n",
    "    print(\"5\")\n",
    "    #print(newImageDescriptor)\n",
    "\n",
    "    # file = open(latentSemanticFileName)\n",
    "    # print(\"6\")\n",
    "\n",
    "    # # reading the data from the file\n",
    "    # with open(latentSemanticFileName) as f:\n",
    "    #     data = f.read()\n",
    "\n",
    "    # dictionaryOfLatentSemantic = json.loads(data)\n",
    "\n",
    "    # descriptorInNewSpace = []\n",
    "\n",
    "    # for i in range(len(dictionaryOfLatentSemantic[\"vt\"])):\n",
    "    #     print(\"prima di coord: \", i)\n",
    "    #     coord = np.asarray(dictionaryOfLatentSemantic[\"vt\"][i]) @ np.asarray(newImageDescriptor)\n",
    "    #     print(\"dopo coord: \", i)\n",
    "    #     print(\"COORD: \", coord)\n",
    "\n",
    "    #     descriptorInNewSpace.append(coord)\n",
    "    #     print(\"dopo append: \", i)\n",
    "\n",
    "    with open(latentSemanticFileName, 'rb') as f:\n",
    "        decomposition = pickle.load(f)\n",
    "\n",
    "    tmp = decomposition.transform([newImageDescriptor])\n",
    "    descriptorInNewSpace = tmp[0]\n",
    "\n",
    "    print(\"ORA STAMPO IL DESCRITTORE RISPETTO ALLE K FEATURES LATENTI\")\n",
    "\n",
    "    print(descriptorInNewSpace)\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open('./databasefilesecondaparte/DB_' + typeOfDescriptor + '.json')\n",
    "    \n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    for chiave, valore in data.items():\n",
    "        splitted = str(chiave).split(\"-\")\n",
    "\n",
    "        if nTask == 6:\n",
    "            key = splitted[1]\n",
    "        else:\n",
    "            key = splitted[2]\n",
    "\n",
    "        dictionaryOfDescriptors[key] = []\n",
    "    \n",
    "    # print(\"SONO ARRIVATO QUA\")\n",
    "    # print(dictionaryOfDescriptors)\n",
    "\n",
    "    for chiave, valore in data.items():\n",
    "        splitted = str(chiave).split(\"-\")\n",
    "\n",
    "        if nTask == 6:\n",
    "            key = splitted[1]\n",
    "        else:\n",
    "            key = splitted[2]\n",
    "\n",
    "        newCurrentDescriptor = decomposition.transform([valore])\n",
    "\n",
    "        tmp = dictionaryOfDescriptors[key]\n",
    "        tmp.append(newCurrentDescriptor)\n",
    "        dictionaryOfDescriptors[key] = tmp\n",
    "\n",
    "    print(\"CICLO SUL DIZIONARIO\")\n",
    "\n",
    "    print(dictionaryOfDescriptors)\n",
    "\n",
    "    print(\"LUNGHEZZA DIZIONARIO: \", len(dictionaryOfDescriptors))\n",
    "\n",
    "    dictionaryOfCentroids = {}\n",
    "\n",
    "    for chiave, value in dictionaryOfDescriptors.items():\n",
    "        npMatrix = np.array(value)\n",
    "        centroide = np.mean(npMatrix, axis=0)\n",
    "        dictionaryOfCentroids[chiave] = centroide\n",
    "\n",
    "    print(\"STAMPO IL DIZIONARIO DEI CENTROIDI\")\n",
    "    print(dictionaryOfCentroids)\n",
    "\n",
    "    bestKey = None\n",
    "    bestValue = None\n",
    "\n",
    "    bestDistance = 99999999999999\n",
    "\n",
    "    for chiave, value in dictionaryOfCentroids.items():\n",
    "        # print(\"STO CICLANDO QUA\")\n",
    "        # print(\"VALUE HA LEN: \", len(value))\n",
    "        # print(\"descriptorInNewSpace HA LEN: \", len(descriptorInNewSpace))\n",
    "\n",
    "        distance = np.linalg.norm(np.array(value) - np.array(descriptorInNewSpace))\n",
    "        \n",
    "        if distance < bestDistance:\n",
    "            bestDistance = distance\n",
    "            bestKey = key\n",
    "            bestValue = value\n",
    "\n",
    "    print(\"LA BEST KEY E': \", bestKey)\n",
    "\n",
    "    return bestKey\n",
    "\n",
    "    # except:\n",
    "        #print(\"Il file di query e/o quello delle semantiche latenti non esiste! Inserire un nome corretto!\")\n",
    "\n",
    "    \n",
    "\n",
    "    # if done:\n",
    "    #     # convertiamo l'immagine in un numpy array\n",
    "    #     data = asarray(image)\n",
    "    #     print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def createSimilarityMatrix(modelName, param):\n",
    "\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    lista = []\n",
    "    #dictionaryOfDescriptors contiene TUTTI i descrittori di TUTTI gli oggetti\n",
    "    if param == 'tipo':\n",
    "        lista = ['cc', 'con', 'emboss', 'jitter', 'neg', 'noise01', 'noise02', 'original', 'poster', 'rot', 'smooth', 'stipple']\n",
    "        print(lista)\n",
    "\n",
    "    elif param == 'soggetto':\n",
    "        lista = []\n",
    "        for i in range(40):\n",
    "            lista.append(str(i)) #aggiungo il soggetto\n",
    "\n",
    "        print(lista)\n",
    "    else: \n",
    "        raise Exception(\"Param ha un valore sbagliato\")\n",
    "        \n",
    "    arrayDiCentroidi = []\n",
    "\n",
    "    filteredFile = []\n",
    "    for i in range(len(lista)):\n",
    "        filteredDictionary = {}\n",
    "        for key in dictionaryOfDescriptors:\n",
    "            if lista[i] in key:\n",
    "                filteredFile.append(key)\n",
    "                filteredDictionary[key] = dictionaryOfDescriptors[key]\n",
    "        \n",
    "        matrix = []\n",
    "        #TODO: calcolare il centroide\n",
    "        for key in filteredDictionary:\n",
    "            matrix.append(filteredDictionary[key])\n",
    "        \n",
    "        npMatrix = np.array(matrix)\n",
    "\n",
    "        centroide = np.mean(npMatrix, axis=0)\n",
    "        arrayDiCentroidi.append(centroide)\n",
    "\n",
    "    print(arrayDiCentroidi)\n",
    "    print(len(arrayDiCentroidi[0]))\n",
    "\n",
    "    #ciclare tutti i centroidi e calcolare le distanze tra di essi\n",
    "\n",
    "    matrixForDecomposition = np.eye(len(arrayDiCentroidi)) #matrice tipo-tipo o soggetto-soggetto\n",
    "\n",
    "    for i in range(len(arrayDiCentroidi)):\n",
    "        for j in range(i, len(arrayDiCentroidi)):\n",
    "            similarity = cosine_similarity(arrayDiCentroidi[i].reshape(1,-1), arrayDiCentroidi[j].reshape(1,-1)) # calcolo la similarità tra i due descrittori\n",
    "            matrixForDecomposition[i][j] = similarity\n",
    "            matrixForDecomposition[j][i] = similarity\n",
    "\n",
    "    print(matrixForDecomposition)\n",
    "    \n",
    "    return matrixForDecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Rank Algorithm\n",
    "\n",
    "Link: https://www.geeksforgeeks.org/page-rank-algorithm-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, alpha=0.85, personalization=None,\n",
    "\t\t\tmax_iter=100, tol=1.0e-6, nstart=None, weight='weight',\n",
    "\t\t\tdangling=None):\n",
    "\t\"\"\"Return the PageRank of the nodes in the graph.\n",
    "\n",
    "\tPageRank computes a ranking of the nodes in the graph G based on\n",
    "\tthe structure of the incoming links. It was originally designed as\n",
    "\tan algorithm to rank web pages.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tG : graph\n",
    "\tA NetworkX graph. Undirected graphs will be converted to a directed\n",
    "\tgraph with two directed edges for each undirected edge.\n",
    "\n",
    "\talpha : float, optional\n",
    "\tDamping parameter for PageRank, default=0.85.\n",
    "\n",
    "\tpersonalization: dict, optional\n",
    "\tThe \"personalization vector\" consisting of a dictionary with a\n",
    "\tkey for every graph node and nonzero personalization value for each node.\n",
    "\tBy default, a uniform distribution is used.\n",
    "\n",
    "\tmax_iter : integer, optional\n",
    "\tMaximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "\ttol : float, optional\n",
    "\tError tolerance used to check convergence in power method solver.\n",
    "\n",
    "\tnstart : dictionary, optional\n",
    "\tStarting value of PageRank iteration for each node.\n",
    "\n",
    "\tweight : key, optional\n",
    "\tEdge data key to use as weight. If None weights are set to 1.\n",
    "\n",
    "\tdangling: dict, optional\n",
    "\tThe outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "\tany outedges. The dict key is the node the outedge points to and the dict\n",
    "\tvalue is the weight of that outedge. By default, dangling nodes are given\n",
    "\toutedges according to the personalization vector (uniform if not\n",
    "\tspecified). This must be selected to result in an irreducible transition\n",
    "\tmatrix (see notes under google_matrix). It may be common to have the\n",
    "\tdangling dict to be the same as the personalization dict.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tpagerank : dictionary\n",
    "\tDictionary of nodes with PageRank as value\n",
    "\n",
    "\tNotes\n",
    "\t-----\n",
    "\tThe eigenvector calculation is done by the power iteration method\n",
    "\tand has no guarantee of convergence. The iteration will stop\n",
    "\tafter max_iter iterations or an error tolerance of\n",
    "\tnumber_of_nodes(G)*tol has been reached.\n",
    "\n",
    "\tThe PageRank algorithm was designed for directed graphs but this\n",
    "\talgorithm does not check if the input graph is directed and will\n",
    "\texecute on undirected graphs by converting each edge in the\n",
    "\tdirected graph to two edges.\n",
    "\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tif len(G) == 0:\n",
    "\t\treturn {}\n",
    "\n",
    "\tif not G.is_directed():\n",
    "\t\tD = G.to_directed()\n",
    "\telse:\n",
    "\t\tD = G\n",
    "\n",
    "\t# Create a copy in (right) stochastic form\n",
    "\tW = nx.stochastic_graph(D, weight=weight)\n",
    "\tN = W.number_of_nodes()\n",
    "\n",
    "\t# Choose fixed starting vector if not given\n",
    "\tif nstart is None:\n",
    "\t\tx = dict.fromkeys(W, 1.0 / N)\n",
    "\telse:\n",
    "\t\t# Normalized nstart vector\n",
    "\t\ts = float(sum(nstart.values()))\n",
    "\t\tx = dict((k, v / s) for k, v in nstart.items())\n",
    "\n",
    "\tif personalization is None:\n",
    "\n",
    "\t\t# Assign uniform personalization vector if not given\n",
    "\t\tp = dict.fromkeys(W, 1.0 / N)\n",
    "\telse:\n",
    "\t\tmissing = set(G) - set(personalization)\n",
    "\t\tif missing:\n",
    "\t\t\traise nx.NetworkXError('Personalization dictionary '\n",
    "\t\t\t\t\t\t\t\t'must have a value for every node. '\n",
    "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing)\n",
    "\t\ts = float(sum(personalization.values()))\n",
    "\t\tp = dict((k, v / s) for k, v in personalization.items())\n",
    "\n",
    "\tif dangling is None:\n",
    "\n",
    "\t\t# Use personalization vector if dangling vector not specified\n",
    "\t\tdangling_weights = p\n",
    "\telse:\n",
    "\t\tmissing = set(G) - set(dangling)\n",
    "\t\tif missing:\n",
    "\t\t\traise nx.NetworkXError('Dangling node dictionary '\n",
    "\t\t\t\t\t\t\t\t'must have a value for every node. '\n",
    "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing)\n",
    "\t\ts = float(sum(dangling.values()))\n",
    "\t\tdangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
    "\tdangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "\n",
    "\t# power iteration: make up to max_iter iterations\n",
    "\tfor _ in range(max_iter):\n",
    "\t\txlast = x\n",
    "\t\tx = dict.fromkeys(xlast.keys(), 0)\n",
    "\t\tdanglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
    "\t\tfor n in x:\n",
    "\n",
    "\t\t\t# this matrix multiply looks odd because it is\n",
    "\t\t\t# doing a left multiply x^T=xlast^T*W\n",
    "\t\t\tfor nbr in W[n]:\n",
    "\t\t\t\tx[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
    "\t\t\tx[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
    "\n",
    "\t\t# check convergence, l1 norm\n",
    "\t\terr = sum([abs(x[n] - xlast[n]) for n in x])\n",
    "\t\tif err < N*tol:\n",
    "\t\t\treturn x\n",
    "\traise nx.NetworkXError('pagerank: power iteration failed to converge '\n",
    "\t\t\t\t\t\t'in %d iterations.' % max_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

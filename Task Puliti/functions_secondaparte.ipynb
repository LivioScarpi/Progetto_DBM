{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2\n",
    "NOTA:\n",
    "- In questa fase, le immagini da inserire nel database saranno etichettate come image-X-Y- Z.png, dove\n",
    "    - X ∈ {cc, con, detail, emboss, jitter, neg, noise1, noise2, original, poster, rot, smooth, stipple} denota il tipo dell’immagine;\n",
    "    - 1≤Y≤40denota ilsubjectID,and\n",
    "    - 1 ≤ Z ≤ 10 denota l’ ID del campione dell’immagine.\n",
    "    \n",
    "\n",
    "- I task in questa fase riguardano i tre modelli di feature e le misure di distanza/similarita’ sviluppate nella precedente.\n",
    " \n",
    "- Potete usare le librerie esistenti per la decomposizione LDA.\n",
    "- Potete usare le librerie esistenti per l’estrazione di autovalori e autovettori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.task1_new import *\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.stats import skew\n",
    "\n",
    "# load and display an image with Matplotlib\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import re\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787\n"
     ]
    }
   ],
   "source": [
    "# import OS module\n",
    "import os\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"./secondaparte/images/\"\n",
    "dir_list = os.listdir(path) # array di stringhe contenente i nomi dei file\n",
    " \n",
    "dir_list.sort()\n",
    "dir_list.remove('.DS_Store')\n",
    "\n",
    "print(len(dir_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryAndWriteOnFile(matrix, filtered_dir_list, passedParameter, strategyName, modelName):\n",
    "    matrixtrasposta = matrix.T\n",
    "\n",
    "    dictionaryFeaturesLatenti = {}\n",
    "    for i in range(len(matrixtrasposta)):\n",
    "        sequenzaFeatureLatente = []\n",
    "        for j in range(len(matrixtrasposta[i])):\n",
    "            splittedString = re.split('-', filtered_dir_list[j])\n",
    "            pair = ()\n",
    "\n",
    "            if passedParameter == 'X':\n",
    "                pair = (splittedString[2], matrixtrasposta[i][j])\n",
    "            elif passedParameter == 'Y':\n",
    "                pair = (splittedString[1], matrixtrasposta[i][j])\n",
    "            \n",
    "            sequenzaFeatureLatente.append(pair)\n",
    "        \n",
    "        sequenzaOrdinata = sorted(sequenzaFeatureLatente, key=lambda x: x[1], reverse=True)\n",
    "        dictionaryFeaturesLatenti['Feature' + str(i)] = sequenzaOrdinata\n",
    "\n",
    "    #print(dictionaryFeaturesLatenti)\n",
    "\n",
    "    fileName = ''\n",
    "\n",
    "    if passedParameter == 'X':\n",
    "        fileName = 'task1_' + modelName + '_' + strategyName + '_coppie_soggetto-peso.txt'\n",
    "    elif passedParameter == 'Y':\n",
    "        fileName = 'task2_' + modelName + '_' + strategyName + '_coppie_tipo-peso.txt'\n",
    "\n",
    "    # controlliamo se il file esiste o no\n",
    "    file = open('./outputfilesecondaparte/' + fileName,'w+')\n",
    "\n",
    "    with open('./outputfilesecondaparte/' + fileName, 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dictionaryFeaturesLatenti, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryAndWriteOnFileTask34(matrix, filtered_dir_list, passedParameter, strategyName, modelName):\n",
    "    matrixtrasposta = matrix.T\n",
    "\n",
    "    print(filtered_dir_list)\n",
    "    print(\"LEN LIST\", len(filtered_dir_list))\n",
    "    print(\"len(matrixtrasposta)\", len(matrixtrasposta))\n",
    "    print(\"len(matrixtrasposta[0])\", len(matrixtrasposta[0]))\n",
    "\n",
    "    dictionaryFeaturesLatenti = {}\n",
    "    for i in range(len(matrixtrasposta)):\n",
    "        sequenzaFeatureLatente = []\n",
    "        for j in range(len(matrixtrasposta[i])):\n",
    "            pair = ()\n",
    "\n",
    "            if passedParameter == 'soggetto':\n",
    "                pair = (filtered_dir_list[j], matrixtrasposta[i][j])\n",
    "            elif passedParameter == 'tipo':\n",
    "                pair = (filtered_dir_list[j], matrixtrasposta[i][j])\n",
    "            \n",
    "            sequenzaFeatureLatente.append(pair)\n",
    "        \n",
    "        sequenzaOrdinata = sorted(sequenzaFeatureLatente, key=lambda x: x[1], reverse=True)\n",
    "        dictionaryFeaturesLatenti['Feature' + str(i)] = sequenzaOrdinata\n",
    "\n",
    "    print(\"DOPO IL FOR\")\n",
    "\n",
    "    #print(dictionaryFeaturesLatenti)\n",
    "\n",
    "    fileName = ''\n",
    "\n",
    "    if passedParameter == 'soggetto':\n",
    "        fileName = 'task4_' + modelName + '_' + strategyName + '_coppie_soggetto-peso.txt'\n",
    "    elif passedParameter == 'tipo':\n",
    "        fileName = 'task3_' + modelName + '_' + strategyName + '_coppie_tipo-peso.txt'\n",
    "\n",
    "    # controlliamo se il file esiste o no\n",
    "    file = open('./outputfilesecondaparte/' + fileName,'w+')\n",
    "\n",
    "    with open('./outputfilesecondaparte/' + fileName, 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dictionaryFeaturesLatenti, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def ottieniKSemanticheLatenti(modelName, param, k, strategyName, passedParameter):\n",
    "    # apriamo l'immagine\n",
    "\n",
    "    # imagePath = 'secondaparte/images/image-' + X + '-1-1.png'\n",
    "    # image = Image.open(imagePath)\n",
    "\n",
    "    # # convertiamo l'immagine in un numpy array\n",
    "    # data = asarray(image)\n",
    "    # print(data)\n",
    "\n",
    "    # plt.imshow(image, cmap =\"gray\")\n",
    "    # plt.show()\n",
    "\n",
    "    filtered_dir_list = []\n",
    "\n",
    "    if passedParameter == 'X':\n",
    "        filtered_dir_list = list(filter(lambda k: param in k, dir_list))\n",
    "    elif passedParameter == 'Y':\n",
    "        for i in range(len(dir_list)):\n",
    "            splittedString = re.split('-', dir_list[i])\n",
    "\n",
    "            if splittedString[2] == param:\n",
    "                filtered_dir_list.append(dir_list[i])\n",
    "\n",
    "    print(\"Numero di file su cui applichiamo i metodi: \", len(filtered_dir_list))\n",
    "    #print(filtered_dir_list)\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Iterating through the json\n",
    "            # list\n",
    "            for key in data:\n",
    "                #print(key)\n",
    "                if key in filtered_dir_list:\n",
    "                    dictionaryOfDescriptors[key] = data[key]\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    matrixObjFeatures = []\n",
    "\n",
    "    lista = []\n",
    "\n",
    "    for key in dictionaryOfDescriptors:\n",
    "        matrixObjFeatures.append(dictionaryOfDescriptors[key])\n",
    "        lista.append(key)\n",
    "\n",
    "    print(\"Lunghezza di un descrittore: \", len(matrixObjFeatures[0]))\n",
    "    print(\"Numero di oggetti: \", len(matrixObjFeatures))\n",
    "    \n",
    "    # calcoliamo il numero massimo di features latenti possibili, ovvero il minimo tra il numero di oggetti e di features di partenza\n",
    "    numberOfFeatures = min(len(matrixObjFeatures[0]), len(matrixObjFeatures))\n",
    "\n",
    "    match strategyName:\n",
    "        case 'PCA':\n",
    "            print(\"SONO IN PCA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(matrixObjFeatures)\n",
    "\n",
    "            covMatr = np.cov(numpyMatrixObjFeatures.T)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di covarianza: \", len(covMatr))\n",
    "            print(\"Numero colonne matrice di covarianza: \", len(covMatr[0]))\n",
    "            print(\"La matrice di covarianza è quadrata? \", len(covMatr) == len(covMatr[0]))\n",
    "\n",
    "\n",
    "            #TODO: RICONTROLLARE BENE\n",
    "            pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data = pca.fit_transform(matrixObjFeatures)\n",
    "            print(\"\\n\")\n",
    "            print(converted_data)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di converted_data: \", len(converted_data))\n",
    "            print(\"Numero colonne matrice di converted_data: \", len(converted_data[0]))\n",
    "\n",
    "            # Questa imlementazione di PCA mi dà una matrice che ha tante righe quanti sono gli oggetti e tante colonne quante sono le features\n",
    "            # mi descrive gli oggetti sulla base delle nuove features latenti\n",
    "\n",
    "            createDictionaryAndWriteOnFile(converted_data, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            return 1\n",
    "\n",
    "        case 'SVD':\n",
    "\n",
    "            print(\"SONO IN SVD\")\n",
    "\n",
    "            image = np.array(matrixObjFeatures)\n",
    "\n",
    "            # TODO: capire se dobbiamo passare già qua il parametro k\n",
    "            u, s, vt = randomized_svd(image, n_components=numberOfFeatures)\n",
    "\n",
    "            print(\"\\nNumero righe di U: \", len(u))\n",
    "            print(\"\\nNumero colonne di U: \", len(u[0]))\n",
    "\n",
    "            matrixS = np.diag(s)\n",
    "            print(\"\\nNumero righe di S: \", len(matrixS))\n",
    "            print(\"\\nNumero colonne di S: \", len(matrixS[0]))\n",
    "\n",
    "            print(\"\\nNumero righe di VT: \", len(vt))\n",
    "            print(\"\\nNumero colonne di VT: \", len(vt[0]))\n",
    "\n",
    "            print(\"\\nLeft Singular Vectors:\")\n",
    "            print(u)\n",
    "            print(\"\\nSingular Values:\")\n",
    "            print(np.diag(s))\n",
    "            print(\"\\nRight Singular Vectors:\")\n",
    "            print(vt)\n",
    "\n",
    "            # SVD\n",
    "            # U, s, VT = svd(image, full_matrices=False) # full_matrices=False\n",
    "            # print(\"STAMPO U che ha lunghezza: \", len(U))\n",
    "\n",
    "            # print(\"STAMPO U dove un oggetto ha lunghezza: \", len(U[0]))\n",
    "            # print(U)\n",
    "\n",
    "            # print(\"STAMPO S dove un oggetto ha lunghezza: \", len(s))\n",
    "            # print(s)\n",
    "\n",
    "            # print(\"STAMPO V trasposta dove un oggetto ha lunghezza: \", len(VT[0]))\n",
    "            # print(VT)\n",
    "\n",
    "            # print(\"STAMPO A\")\n",
    "\n",
    "            # Traspongo la matrice U per poter iterare sulle righe e quindi costruire la sequenza di coppie soggetto-peso per ogni feature latente\n",
    "\n",
    "            createDictionaryAndWriteOnFile(u, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            return 2\n",
    "\n",
    "        case 'LDA':\n",
    "            print(\"SONO IN LDA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(matrixObjFeatures)\n",
    "            lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "            #lda.fit(numpyMatrixObjFeatures)\n",
    "            #LatentDirichletAllocation(...)\n",
    "            # get topics for some given samples:\n",
    "            result = lda.fit_transform(numpyMatrixObjFeatures)\n",
    "            print(result)\n",
    "            print(\"LUNGHEZZA RESULT\")\n",
    "            print(\"righe: \", len(result))\n",
    "            print(\"colonne: \", len(result[0]))\n",
    "\n",
    "            # capire come fare LDA\n",
    "            # # Define the number of topics or components\n",
    "            # num_components=5\n",
    "\n",
    "            # # Create LDA object\n",
    "            # model=LatentDirichletAllocation(n_components=num_components)\n",
    "\n",
    "            # # Fit and Transform SVD model on data\n",
    "            # lda_matrix = model.fit_transform(matrixObjFeatures)\n",
    "\n",
    "            # # Get Components \n",
    "            # lda_components=model.components_\n",
    "\n",
    "            # for index, component in enumerate(lda_components):\n",
    "            #     print(index)\n",
    "            #     print(component)\n",
    "\n",
    "            print(\"FINISCO LDA\")\n",
    "\n",
    "            #TODO: CAPIRE COME INTERPRETARE QUESTA MATRICE RESULT DA LDA\n",
    "            createDictionaryAndWriteOnFile(result, lista, passedParameter, strategyName, modelName)\n",
    "\n",
    "            print(\"FINISCO SCRITTURA SU FILE\")\n",
    "\n",
    "            # # define transform\n",
    "            # lda = LinearDiscriminantAnalysis()\n",
    "            # # prepare transform on dataset\n",
    "            # lda.fit(matrixObjFeatures)\n",
    "            # # apply transform to dataset\n",
    "            # transformed = lda.transform(matrixObjFeatures)\n",
    "\n",
    "            # print(\"transformed\")\n",
    "            # print(transformed)\n",
    "\n",
    "            # # # define the pipeline\n",
    "            # # steps = [('lda', LinearDiscriminantAnalysis()), ('m', GaussianNB())]\n",
    "\n",
    "            # # # It can also be a good idea to standardize data prior to performing the LDA transform if the input variables have differing units or scales; for example:\n",
    "            # # #steps = [('s', StandardScaler()), ('lda', LinearDiscriminantAnalysis()), ('m', GaussianNB())]\n",
    "            # # model = Pipeline(steps=steps)\n",
    "            return 3\n",
    "\n",
    "        case _:\n",
    "            return 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 3 e 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def ottieniKSemanticheLatentiTask34(modelName, param, k, strategyName):\n",
    "\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    lista = []\n",
    "    #dictionaryOfDescriptors contiene TUTTI i descrittori di TUTTI gli oggetti\n",
    "    if param == 'tipo':\n",
    "        lista = ['cc', 'con', 'emboss', 'jitter', 'neg', 'noise01', 'noise02', 'original', 'poster', 'rot', 'smooth', 'stipple']\n",
    "        print(lista)\n",
    "\n",
    "    elif param == 'soggetto':\n",
    "        lista = []\n",
    "        for i in range(40):\n",
    "            lista.append(str(i)) #aggiungo il soggetto\n",
    "\n",
    "        print(lista)\n",
    "    else: \n",
    "        raise Exception(\"Param ha un valore sbagliato\")\n",
    "        \n",
    "    arrayDiCentroidi = []\n",
    "\n",
    "    filteredFile = []\n",
    "    for i in range(len(lista)):\n",
    "        filteredDictionary = {}\n",
    "        for key in dictionaryOfDescriptors:\n",
    "            if lista[i] in key:\n",
    "                filteredFile.append(key)\n",
    "                filteredDictionary[key] = dictionaryOfDescriptors[key]\n",
    "        \n",
    "        matrix = []\n",
    "        #TODO: calcolare il centroide\n",
    "        for key in filteredDictionary:\n",
    "            matrix.append(filteredDictionary[key])\n",
    "        \n",
    "        npMatrix = np.array(matrix)\n",
    "\n",
    "        centroide = np.mean(npMatrix, axis=0)\n",
    "        arrayDiCentroidi.append(centroide)\n",
    "\n",
    "    print(arrayDiCentroidi)\n",
    "    print(len(arrayDiCentroidi[0]))\n",
    "\n",
    "    #ciclare tutti i centroidi e calcolare le distanze tra di essi\n",
    "\n",
    "    matrixForDecomposition = np.eye(len(arrayDiCentroidi)) #matrice tipo-tipo o soggetto-soggetto\n",
    "\n",
    "    for i in range(len(arrayDiCentroidi)):\n",
    "        for j in range(i, len(arrayDiCentroidi)):\n",
    "            similarity = cosine_similarity(arrayDiCentroidi[i].reshape(1,-1), arrayDiCentroidi[j].reshape(1,-1)) # calcolo la similarità tra i due descrittori\n",
    "            matrixForDecomposition[i][j] = similarity\n",
    "            matrixForDecomposition[j][i] = similarity\n",
    "\n",
    "    print(matrixForDecomposition)\n",
    "    print(\"QUA 0\")\n",
    "\n",
    "    numpyMatrixForDecomposition = np.array(matrixForDecomposition)\n",
    "    print(\"QUA 1\")\n",
    "\n",
    "    # calcoliamo il numero massimo di features latenti possibili, ovvero il minimo tra il numero di oggetti e di features di partenza\n",
    "    numberOfFeatures = min(len(numpyMatrixForDecomposition[0]), len(numpyMatrixForDecomposition))\n",
    "    \n",
    "    # print(\"FILTERED FILE LEN: \", len(filteredFile))\n",
    "    # print(filteredFile)\n",
    "\n",
    "    print(\"PRIMA DELLO SWITCH\")\n",
    "\n",
    "    match strategyName:\n",
    "        case 'PCA':\n",
    "            print(\"SONO IN PCA\")\n",
    "\n",
    "            covMatr = np.cov(numpyMatrixForDecomposition.T)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di covarianza: \", len(covMatr))\n",
    "            print(\"Numero colonne matrice di covarianza: \", len(covMatr[0]))\n",
    "            print(\"La matrice di covarianza è quadrata? \", len(covMatr) == len(covMatr[0]))\n",
    "\n",
    "            #TODO: CONTROLLARE BENEEEE\n",
    "            pca = PCA(n_components = numberOfFeatures)\n",
    "            converted_data = pca.fit_transform(numpyMatrixForDecomposition)\n",
    "            print(\"\\n\")\n",
    "            print(converted_data)\n",
    "\n",
    "            print(\"\\nNumero righe matrice di converted_data: \", len(converted_data))\n",
    "            print(\"Numero colonne matrice di converted_data: \", len(converted_data[0]))\n",
    "\n",
    "            # Questa imlementazione di PCA mi dà una matrice che ha tante righe quanti sono gli oggetti e tante colonne quante sono le features\n",
    "            # mi descrive gli oggetti sulla base delle nuove features latenti\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(converted_data, lista, param, strategyName, modelName)\n",
    "\n",
    "            return 1\n",
    "\n",
    "        case 'SVD':\n",
    "\n",
    "            print(\"SONO IN SVD\")\n",
    "\n",
    "            image = np.array(numpyMatrixForDecomposition)\n",
    "\n",
    "            # TODO: capire se dobbiamo passare già qua il parametro k\n",
    "            u, s, vt = randomized_svd(image, n_components=numberOfFeatures)\n",
    "\n",
    "            print(\"\\nNumero righe di U: \", len(u))\n",
    "            print(\"\\nNumero colonne di U: \", len(u[0]))\n",
    "\n",
    "            matrixS = np.diag(s)\n",
    "            print(\"\\nNumero righe di S: \", len(matrixS))\n",
    "            print(\"\\nNumero colonne di S: \", len(matrixS[0]))\n",
    "\n",
    "            print(\"\\nNumero righe di VT: \", len(vt))\n",
    "            print(\"\\nNumero colonne di VT: \", len(vt[0]))\n",
    "\n",
    "            print(\"\\nLeft Singular Vectors:\")\n",
    "            print(u)\n",
    "            print(\"\\nSingular Values:\")\n",
    "            print(np.diag(s))\n",
    "            print(\"\\nRight Singular Vectors:\")\n",
    "            print(vt)\n",
    "\n",
    "            # SVD\n",
    "            # U, s, VT = svd(image, full_matrices=False) # full_matrices=False\n",
    "            # print(\"STAMPO U che ha lunghezza: \", len(U))\n",
    "\n",
    "            # print(\"STAMPO U dove un oggetto ha lunghezza: \", len(U[0]))\n",
    "            # print(U)\n",
    "\n",
    "            # print(\"STAMPO S dove un oggetto ha lunghezza: \", len(s))\n",
    "            # print(s)\n",
    "\n",
    "            # print(\"STAMPO V trasposta dove un oggetto ha lunghezza: \", len(VT[0]))\n",
    "            # print(VT)\n",
    "\n",
    "            # print(\"STAMPO A\")\n",
    "\n",
    "            # Traspongo la matrice U per poter iterare sulle righe e quindi costruire la sequenza di coppie soggetto-peso per ogni feature latente\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(u, lista, param, strategyName, modelName)\n",
    "\n",
    "            return 2\n",
    "\n",
    "        case 'LDA':\n",
    "            print(\"SONO IN LDA\")\n",
    "\n",
    "            numpyMatrixObjFeatures = np.array(numpyMatrixForDecomposition)\n",
    "            lda = LatentDirichletAllocation(n_components=numberOfFeatures,\n",
    "                random_state=0)\n",
    "            \n",
    "            result = lda.fit_transform(numpyMatrixObjFeatures)\n",
    "            print(result)\n",
    "            print(\"LUNGHEZZA RESULT\")\n",
    "            print(\"righe: \", len(result))\n",
    "            print(\"colonne: \", len(result[0]))\n",
    "\n",
    "\n",
    "            # capire come fare LDA\n",
    "            # # Define the number of topics or components\n",
    "            # num_components=5\n",
    "\n",
    "            # # Create LDA object\n",
    "            # model=LatentDirichletAllocation(n_components=num_components)\n",
    "\n",
    "            # # Fit and Transform SVD model on data\n",
    "            # lda_matrix = model.fit_transform(matrixObjFeatures)\n",
    "\n",
    "            # # Get Components \n",
    "            # lda_components=model.components_\n",
    "\n",
    "            # for index, component in enumerate(lda_components):\n",
    "            #     print(index)\n",
    "            #     print(component)\n",
    "\n",
    "            print(\"FINISCO LDA\")\n",
    "\n",
    "            createDictionaryAndWriteOnFileTask34(result, lista, param, strategyName, modelName)\n",
    "            print(\"FINISCO SCRITTURA SU FILE\")\n",
    "\n",
    "\n",
    "            # # define transform\n",
    "            # lda = LinearDiscriminantAnalysis()\n",
    "            # # prepare transform on dataset\n",
    "            # lda.fit(matrixObjFeatures)\n",
    "            # # apply transform to dataset\n",
    "            # transformed = lda.transform(matrixObjFeatures)\n",
    "\n",
    "            # print(\"transformed\")\n",
    "            # print(transformed)\n",
    "\n",
    "            # # # define the pipeline\n",
    "            # # steps = [('lda', LinearDiscriminantAnalysis()), ('m', GaussianNB())]\n",
    "\n",
    "            # # # It can also be a good idea to standardize data prior to performing the LDA transform if the input variables have differing units or scales; for example:\n",
    "            # # #steps = [('s', StandardScaler()), ('lda', LinearDiscriminantAnalysis()), ('m', GaussianNB())]\n",
    "            # # model = Pipeline(steps=steps)\n",
    "            return 3\n",
    "\n",
    "        case _:\n",
    "            raise Exception(\"Metodo di decomposizione non valido\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Soluzione scelta:** Per ottenere le matrici di similarità tipo-tipo o soggetto-soggetto abbiamo recuperato tutte le immagini di quel tipo o di quel soggetto a seconda del caso e abbiamo calcolato per ogni tipo o soggetto un centroide.\n",
    "Grazie a questi centroidi abbiamo poi costruito la matrice di similarità sfruttando la cosine similarity.\n",
    "\n",
    "> **Nota bene:** Questa soluzione implementata per costruire la matrice di similarità è solo una delle tante disponibili. Avremmo potuto usare i medoidi oppure costruire una matrice per ogni tipo o soggetto e poi confrontare nella maniera corretta (perchè il confronto di matrici è soggetto all'ordine degli oggetti in esse) le matrici e calcolarne la distanza/similarit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNsimilarImages(imageQueryFile, latentSemanticFileName):\n",
    "\n",
    "    done = False\n",
    "    try:\n",
    "        # apriamo l'immagine\n",
    "        image = Image.open('./secondaparte/images/' + imageQueryFile)\n",
    "        file = open(latentSemanticFileName)\n",
    "        done = True\n",
    "    except:\n",
    "        print(\"Il file di query e/o quello delle semantiche latenti non esiste! Inserire un nome corretto!\")\n",
    "\n",
    "\n",
    "    if done:\n",
    "        # convertiamo l'immagine in un numpy array\n",
    "        data = asarray(image)\n",
    "        print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione usata per ottenre le top-k semantiche latenti.\n",
    "# parametri della funzione:\n",
    "# modelName: nome del modello da usare (ColorMoments, LBP, HOG)\n",
    "# X: il tipo dell'immagine\n",
    "# k: numero delle semantiche da estrarre\n",
    "# strategyName: nome della tecnica con cui su vuole effettuare la riduzione di dimensionalità (PCA, SVD, LDA)\n",
    "\n",
    "def createSimilarityMatrix(modelName, param):\n",
    "\n",
    "    dictionaryOfDescriptors = {}\n",
    "\n",
    "    match modelName:\n",
    "        case 'ColorMoments':\n",
    "            print(\"SONO IN COLOR MOMENTS\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_ColorMoments.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'LBP':\n",
    "            print(\"SONO IN LBP\")\n",
    "            # salvataggio dei descrittori dei color moments\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_LBP.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case 'HOG':\n",
    "            \n",
    "            print(\"HOG\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open('./databasefilesecondaparte/DB_HOG.json')\n",
    "            \n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            dictionaryOfDescriptors = json.load(f)\n",
    "            \n",
    "            # Closing file\n",
    "            f.close()\n",
    "\n",
    "        case _:\n",
    "            return 4\n",
    "            \n",
    "    lista = []\n",
    "    #dictionaryOfDescriptors contiene TUTTI i descrittori di TUTTI gli oggetti\n",
    "    if param == 'tipo':\n",
    "        lista = ['cc', 'con', 'emboss', 'jitter', 'neg', 'noise01', 'noise02', 'original', 'poster', 'rot', 'smooth', 'stipple']\n",
    "        print(lista)\n",
    "\n",
    "    elif param == 'soggetto':\n",
    "        lista = []\n",
    "        for i in range(40):\n",
    "            lista.append(str(i)) #aggiungo il soggetto\n",
    "\n",
    "        print(lista)\n",
    "    else: \n",
    "        raise Exception(\"Param ha un valore sbagliato\")\n",
    "        \n",
    "    arrayDiCentroidi = []\n",
    "\n",
    "    filteredFile = []\n",
    "    for i in range(len(lista)):\n",
    "        filteredDictionary = {}\n",
    "        for key in dictionaryOfDescriptors:\n",
    "            if lista[i] in key:\n",
    "                filteredFile.append(key)\n",
    "                filteredDictionary[key] = dictionaryOfDescriptors[key]\n",
    "        \n",
    "        matrix = []\n",
    "        #TODO: calcolare il centroide\n",
    "        for key in filteredDictionary:\n",
    "            matrix.append(filteredDictionary[key])\n",
    "        \n",
    "        npMatrix = np.array(matrix)\n",
    "\n",
    "        centroide = np.mean(npMatrix, axis=0)\n",
    "        arrayDiCentroidi.append(centroide)\n",
    "\n",
    "    print(arrayDiCentroidi)\n",
    "    print(len(arrayDiCentroidi[0]))\n",
    "\n",
    "    #ciclare tutti i centroidi e calcolare le distanze tra di essi\n",
    "\n",
    "    matrixForDecomposition = np.eye(len(arrayDiCentroidi)) #matrice tipo-tipo o soggetto-soggetto\n",
    "\n",
    "    for i in range(len(arrayDiCentroidi)):\n",
    "        for j in range(i, len(arrayDiCentroidi)):\n",
    "            similarity = cosine_similarity(arrayDiCentroidi[i].reshape(1,-1), arrayDiCentroidi[j].reshape(1,-1)) # calcolo la similarità tra i due descrittori\n",
    "            matrixForDecomposition[i][j] = similarity\n",
    "            matrixForDecomposition[j][i] = similarity\n",
    "\n",
    "    print(matrixForDecomposition)\n",
    "    \n",
    "    return matrixForDecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Rank Algorithm\n",
    "\n",
    "Link: https://www.geeksforgeeks.org/page-rank-algorithm-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, alpha=0.85, personalization=None,\n",
    "\t\t\tmax_iter=100, tol=1.0e-6, nstart=None, weight='weight',\n",
    "\t\t\tdangling=None):\n",
    "\t\"\"\"Return the PageRank of the nodes in the graph.\n",
    "\n",
    "\tPageRank computes a ranking of the nodes in the graph G based on\n",
    "\tthe structure of the incoming links. It was originally designed as\n",
    "\tan algorithm to rank web pages.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tG : graph\n",
    "\tA NetworkX graph. Undirected graphs will be converted to a directed\n",
    "\tgraph with two directed edges for each undirected edge.\n",
    "\n",
    "\talpha : float, optional\n",
    "\tDamping parameter for PageRank, default=0.85.\n",
    "\n",
    "\tpersonalization: dict, optional\n",
    "\tThe \"personalization vector\" consisting of a dictionary with a\n",
    "\tkey for every graph node and nonzero personalization value for each node.\n",
    "\tBy default, a uniform distribution is used.\n",
    "\n",
    "\tmax_iter : integer, optional\n",
    "\tMaximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "\ttol : float, optional\n",
    "\tError tolerance used to check convergence in power method solver.\n",
    "\n",
    "\tnstart : dictionary, optional\n",
    "\tStarting value of PageRank iteration for each node.\n",
    "\n",
    "\tweight : key, optional\n",
    "\tEdge data key to use as weight. If None weights are set to 1.\n",
    "\n",
    "\tdangling: dict, optional\n",
    "\tThe outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "\tany outedges. The dict key is the node the outedge points to and the dict\n",
    "\tvalue is the weight of that outedge. By default, dangling nodes are given\n",
    "\toutedges according to the personalization vector (uniform if not\n",
    "\tspecified). This must be selected to result in an irreducible transition\n",
    "\tmatrix (see notes under google_matrix). It may be common to have the\n",
    "\tdangling dict to be the same as the personalization dict.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tpagerank : dictionary\n",
    "\tDictionary of nodes with PageRank as value\n",
    "\n",
    "\tNotes\n",
    "\t-----\n",
    "\tThe eigenvector calculation is done by the power iteration method\n",
    "\tand has no guarantee of convergence. The iteration will stop\n",
    "\tafter max_iter iterations or an error tolerance of\n",
    "\tnumber_of_nodes(G)*tol has been reached.\n",
    "\n",
    "\tThe PageRank algorithm was designed for directed graphs but this\n",
    "\talgorithm does not check if the input graph is directed and will\n",
    "\texecute on undirected graphs by converting each edge in the\n",
    "\tdirected graph to two edges.\n",
    "\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tif len(G) == 0:\n",
    "\t\treturn {}\n",
    "\n",
    "\tif not G.is_directed():\n",
    "\t\tD = G.to_directed()\n",
    "\telse:\n",
    "\t\tD = G\n",
    "\n",
    "\t# Create a copy in (right) stochastic form\n",
    "\tW = nx.stochastic_graph(D, weight=weight)\n",
    "\tN = W.number_of_nodes()\n",
    "\n",
    "\t# Choose fixed starting vector if not given\n",
    "\tif nstart is None:\n",
    "\t\tx = dict.fromkeys(W, 1.0 / N)\n",
    "\telse:\n",
    "\t\t# Normalized nstart vector\n",
    "\t\ts = float(sum(nstart.values()))\n",
    "\t\tx = dict((k, v / s) for k, v in nstart.items())\n",
    "\n",
    "\tif personalization is None:\n",
    "\n",
    "\t\t# Assign uniform personalization vector if not given\n",
    "\t\tp = dict.fromkeys(W, 1.0 / N)\n",
    "\telse:\n",
    "\t\tmissing = set(G) - set(personalization)\n",
    "\t\tif missing:\n",
    "\t\t\traise nx.NetworkXError('Personalization dictionary '\n",
    "\t\t\t\t\t\t\t\t'must have a value for every node. '\n",
    "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing)\n",
    "\t\ts = float(sum(personalization.values()))\n",
    "\t\tp = dict((k, v / s) for k, v in personalization.items())\n",
    "\n",
    "\tif dangling is None:\n",
    "\n",
    "\t\t# Use personalization vector if dangling vector not specified\n",
    "\t\tdangling_weights = p\n",
    "\telse:\n",
    "\t\tmissing = set(G) - set(dangling)\n",
    "\t\tif missing:\n",
    "\t\t\traise nx.NetworkXError('Dangling node dictionary '\n",
    "\t\t\t\t\t\t\t\t'must have a value for every node. '\n",
    "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing)\n",
    "\t\ts = float(sum(dangling.values()))\n",
    "\t\tdangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
    "\tdangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "\n",
    "\t# power iteration: make up to max_iter iterations\n",
    "\tfor _ in range(max_iter):\n",
    "\t\txlast = x\n",
    "\t\tx = dict.fromkeys(xlast.keys(), 0)\n",
    "\t\tdanglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
    "\t\tfor n in x:\n",
    "\n",
    "\t\t\t# this matrix multiply looks odd because it is\n",
    "\t\t\t# doing a left multiply x^T=xlast^T*W\n",
    "\t\t\tfor nbr in W[n]:\n",
    "\t\t\t\tx[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
    "\t\t\tx[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
    "\n",
    "\t\t# check convergence, l1 norm\n",
    "\t\terr = sum([abs(x[n] - xlast[n]) for n in x])\n",
    "\t\tif err < N*tol:\n",
    "\t\t\treturn x\n",
    "\traise nx.NetworkXError('pagerank: power iteration failed to converge '\n",
    "\t\t\t\t\t\t'in %d iterations.' % max_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
